import requests
from bs4 import BeautifulSoup
import re
import argparse
import json
import concurrent.futures
import time
from parse_exploits import parse_exploits

# Define the user agent string
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"

def scan_sql_injection(url):
    payloads = ["' OR '1'='1", "' OR '1'='1' --", "' OR '1'='1' /*", "' OR '1'='1' #", "' OR '1'='2", "' OR '1'='3"]
    error_messages = ["error", "syntax", "SQL", "mysql", "sql", "database", "query", "exception"]
    for payload in payloads:
        try:
            test_url = f"{url}?test={payload}"
            response = requests.get(test_url, headers={"User-Agent": USER_AGENT})
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            if any(error_message in response.text.lower() for error_message in error_messages):
                return True, response_time, response_code
        except requests.RequestException:
            return False, 0, 0
    return False, 0, 0

def scan_xss(url):
    payloads = ["<script>alert('XSS')</script>", "<img src=x onerror=alert('XSS')>", "<body onload=alert('XSS')>", "<svg onload=alert('XSS')>", "<iframe src=javascript:alert('XSS')>"]
    for payload in payloads:
        try:
            test_url = f"{url}?test={payload}"
            response = requests.get(test_url, headers={"User-Agent": USER_AGENT})
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            if payload in response.text:
                return True, response_time, response_code
        except requests.RequestException:
            return False, 0, 0
    return False, 0, 0

def scan_security_misconfigurations(url):
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        headers = response.headers
        if 'X-Content-Type-Options' not in headers or 'X-Frame-Options' not in headers or 'Content-Security-Policy' not in headers or 'Strict-Transport-Security' not in headers or 'Referrer-Policy' not in headers or 'Permissions-Policy' not in headers:
            return True, response_time, response_code
    except requests.RequestException:
        return False, 0, 0
    return False, 0, 0

def scan_broken_authentication(url, login_url=None):
    """
    Scans the given URL for Broken Authentication vulnerabilities by checking for default credentials, weak lockout mechanisms, and session management flaws.
    """
    results = []
    default_credentials = [("admin", "admin"), ("admin", "password"), ("user", "user")]
    login_url = login_url or url.rstrip('/') + "/login"
    for username, password in default_credentials:
        try:
            response = requests.post(login_url, data={"username": username, "password": password}, headers={"User-Agent": USER_AGENT}, allow_redirects=False)
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            cookies = response.headers.get('Set-Cookie')
            if response.status_code == 200 and cookies:
                remediation = "Implement strong authentication mechanisms and avoid using default credentials. Example in Python using Flask:\n\nfrom flask import Flask, request\napp = Flask(__name__)\n@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form['username']\n    password = request.form['password']\n    if username == 'admin' and password == 'admin':\n        return 'Default credentials are not allowed'\n    return 'Login successful'\n"
                results.append({"type": "Broken Authentication", "url": login_url, "credentials": (username, password), "status": "Default credentials accepted", "remediation": remediation, "response_time": response_time, "response_code": response_code})
                return results
            elif response.status_code in [301, 302] and "Location" in response.headers and cookies:
                remediation = "Implement strong authentication mechanisms and avoid using default credentials. Example in Python using Flask:\n\nfrom flask import Flask, request\napp = Flask()\n@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form['username']\n    password = request.form['password']\n    if username == 'admin' and password == 'admin':\n        return 'Default credentials are not allowed'\n    return 'Login successful'\n"
                results.append({"type": "Broken Authentication", "url": login_url, "credentials": (username, password), "status": "Default credentials accepted", "remediation": remediation, "response_time": response_time, "response_code": response_code})
                return results
        except requests.RequestException as e:
            results.append({"type": "Broken Authentication", "url": login_url, "credentials": (username, password), "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    results.append({"type": "Broken Authentication", "url": login_url, "status": "No default credentials accepted", "response_time": 0, "response_code": 0})
    return results

def scan_session_management(url):
    """
    Scans the given URL for Session Management Schema vulnerabilities by checking for common session management flaws.
    """
    results = []
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        cookies = response.headers.get('Set-Cookie')
        if not cookies:
            results.append({"type": "Session Management", "url": url, "status": "No session cookies found", "response_time": response_time, "response_code": response_code})
        else:
            for cookie in cookies.split(','):
                cookie_name = cookie.split('=')[0].strip()
                if 'secure' not in cookie.lower():
                    results.append({"type": "Session Management", "url": url, "cookie": cookie_name, "status": "Cookie not marked as secure", "response_time": response_time, "response_code": response_code})
                if 'httponly' not in cookie.lower():
                    results.append({"type": "Session Management", "url": url, "cookie": cookie_name, "status": "Cookie not marked as HttpOnly", "response_time": response_time, "response_code": response_code})
    except requests.RequestException as e:
        results.append({"type": "Session Management", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_vulnerable_components(url):
    """
    Scans the given URL for outdated or vulnerable third-party components.
    """
    results = []
    known_vulnerable_components = {
        "jquery": ["1.12.4", "2.2.4"],
        "bootstrap": ["3.3.7", "4.0.0"],
        "angular": ["1.5.8", "1.6.6"]
    }
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        page_content = response.text
        for component, versions in known_vulnerable_components.items():
            for version in versions:
                if f"{component}-{version}" in page_content:
                    results.append({"type": "Vulnerable Component", "url": url, "component": component, "version": version, "status": "Vulnerable version detected", "severity": "High", "recommendation": f"Update {component} to a secure version.", "response_time": response_time, "response_code": response_code})
    except requests.RequestException as e:
        results.append({"type": "Vulnerable Component", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_nmap(url):
    """
    Scans the given URL for open ports and services using Nmap.
    """
    import subprocess
    import time
    results = []
    try:
        # Use the Docker container's IP address directly
        docker_ip = "172.17.0.4"
        # Run Nmap scan
        nmap_command = ["nmap", "-sV", "-p", "9090", "-Pn", docker_ip]
        start_time = time.time()
        nmap_output = subprocess.check_output(nmap_command).decode('utf-8')
        response_time = time.time() - start_time
        results.append({"type": "Nmap Scan", "url": url, "output": nmap_output, "status": "Completed", "response_time": response_time, "response_code": 0})
    except subprocess.CalledProcessError as e:
        response_time = time.time() - start_time
        results.append({"type": "Nmap Scan", "url": url, "status": f"Error: {e}", "response_time": response_time, "response_code": e.returncode})
    return results

def information_gathering(url):
    """
    Performs information gathering tasks such as search engine discovery, web server fingerprinting, and application enumeration on the given URL.
    """
    results = []
    try:
        # Commented out to avoid unauthorized external service access
        # Search Engine Discovery
        # search_engine_url = f"https://www.google.com/search?q=site:{url}"
        # response = requests.get(search_engine_url, headers={"User-Agent": USER_AGENT})
        # response_time = response.elapsed.total_seconds()
        # response_code = response.status_code
        # if response.status_code == 200:
        #     results.append({"type": "Information Gathering", "url": url, "method": "Search Engine Discovery", "status": "Success", "response_time": response_time, "response_code": response_code})
        # else:
        #     results.append({"type": "Information Gathering", "url": url, "method": "Search Engine Discovery", "status": "Failed", "response_time": response_time, "response_code": response_code})

        # Fingerprint Web Server
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        server_header = response.headers.get('Server')
        if server_header:
            results.append({"type": "Information Gathering", "url": url, "method": "Fingerprint Web Server", "server": server_header, "status": "Success", "response_time": response_time, "response_code": response_code})
        else:
            results.append({"type": "Information Gathering", "url": url, "method": "Fingerprint Web Server", "status": "Failed", "response_time": response_time, "response_code": response_code})

        # Enumerate Applications on Web Server
        common_paths = ["/admin", "/login", "/dashboard", "/api"]
        for path in common_paths:
            full_url = url.rstrip('/') + path
            response = requests.get(full_url, headers={"User-Agent": USER_AGENT})
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            if response.status_code == 200:
                results.append({"type": "Information Gathering", "url": full_url, "method": "Enumerate Applications", "status": "Found", "response_time": response_time, "response_code": response_code})
            else:
                results.append({"type": "Information Gathering", "url": full_url, "method": "Enumerate Applications", "status": "Not Found", "response_time": response_time, "response_code": response_code})
    except requests.RequestException as e:
        results.append({"type": "Information Gathering", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_identification_and_authentication_failures(url):
    """
    Scans the given URL for Identification and Authentication Failures by checking for weak authentication mechanisms.
    """
    results = []
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        if response.status_code == 200:
            if "login" in response.text.lower():
                results.append({"type": "Identification and Authentication Failures", "url": url, "status": "Potential vulnerability found", "severity": "High", "recommendation": "Implement strong authentication mechanisms.", "response_time": response_time, "response_code": response_code})
        else:
            results.append({"type": "Identification and Authentication Failures", "url": url, "status": "No vulnerability found", "response_time": response_time, "response_code": response_code})
    except requests.RequestException as e:
        results.append({"type": "Identification and Authentication Failures", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_software_and_data_integrity_failures(url):
    """
    Scans the given URL for Software and Data Integrity Failures by checking for the use of known vulnerable libraries.
    """
    results = []
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        if response.status_code == 200:
            if "jquery-1.12.4" in response.text:
                results.append({"type": "Software and Data Integrity Failures", "url": url, "library": "jQuery 1.12.4", "status": "Vulnerable library found", "severity": "High", "recommendation": "Update jQuery to a secure version.", "response_time": response_time, "response_code": response_code})
        else:
            results.append({"type": "Software and Data Integrity Failures", "url": url, "status": "No vulnerability found", "response_time": response_time, "response_code": response_code})
    except requests.RequestException as e:
        results.append({"type": "Software and Data Integrity Failures", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_security_logging_and_monitoring_failures(url):
    """
    Scans the given URL for Security Logging and Monitoring Failures by checking for the presence of security logs.
    """
    results = []
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        if response.status_code == 200:
            if "log" in response.text.lower():
                results.append({"type": "Security Logging and Monitoring Failures", "url": url, "status": "Potential vulnerability found", "severity": "Medium", "recommendation": "Implement comprehensive security logging and monitoring.", "response_time": response_time, "response_code": response_code})
        else:
            results.append({"type": "Security Logging and Monitoring Failures", "url": url, "status": "No vulnerability found", "response_time": response_time, "response_code": response_code})
    except requests.RequestException as e:
        results.append({"type": "Security Logging and Monitoring Failures", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_ssrf(url):
    """
    Scans the given URL for Server-Side Request Forgery (SSRF) vulnerabilities.
    """
    results = []
    payloads = ["http://localhost:9090", "http://127.0.0.1:9090"]
    for payload in payloads:
        try:
            test_url = f"{url}?url={payload}"
            response = requests.get(test_url, headers={"User-Agent": USER_AGENT})
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            if response.status_code == 200 and "localhost" in response.text:
                results.append({"type": "SSRF", "url": test_url, "payload": payload, "status": "Potential vulnerability found", "severity": "High", "recommendation": "Validate and sanitize user input for URLs.", "response_time": response_time, "response_code": response_code})
                return results
        except requests.RequestException as e:
            results.append({"type": "SSRF", "url": test_url, "payload": payload, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    results.append({"type": "SSRF", "url": url, "status": "No vulnerability found", "response_time": 0, "response_code": 0})
    return results

def scan_api_vulnerabilities(url):
    """
    Scans the given URL for API vulnerabilities by testing both REST and GraphQL APIs.
    """
    results = []
    rest_payloads = [
        {"method": "GET", "endpoint": "/api/v1/resource", "params": {"id": "1 OR 1=1"}},
        {"method": "POST", "endpoint": "/api/v1/resource", "data": {"name": "<script>alert('XSS')</script>"}}
    ]
    graphql_payloads = [
        {"query": "{ user(id: \"1 OR 1=1\") { id name } }"},
        {"query": "mutation { createUser(name: \"<script>alert('XSS')</script>\") { id name } }"}
    ]

    for payload in rest_payloads:
        try:
            if payload["method"] == "GET":
                response = requests.get(url + payload["endpoint"], params=payload["params"], headers={"User-Agent": USER_AGENT})
            elif payload["method"] == "POST":
                response = requests.post(url + payload["endpoint"], data=payload["data"], headers={"User-Agent": USER_AGENT})
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            if "error" in response.text or "syntax" in response.text or "SQL" in response.text or "<script>alert('XSS')</script>" in response.text:
                results.append({"type": "API Vulnerability", "url": url + payload["endpoint"], "payload": payload, "status": "Potential vulnerability found", "severity": "High", "recommendation": "Sanitize and validate API inputs.", "response_time": response_time, "response_code": response_code})
        except requests.RequestException as e:
            results.append({"type": "API Vulnerability", "url": url + payload["endpoint"], "payload": payload, "status": f"Error: {e}", "response_time": 0, "response_code": 0})

    for payload in graphql_payloads:
        try:
            response = requests.post(url, json=payload, headers={"User-Agent": USER_AGENT})
            response_time = response.elapsed.total_seconds()
            response_code = response.status_code
            if "error" in response.text or "syntax" in response.text or "SQL" in response.text or "<script>alert('XSS')" in response.text:
                results.append({"type": "API Vulnerability", "url": url, "payload": payload, "status": "Potential vulnerability found", "severity": "High", "recommendation": "Sanitize and validate GraphQL inputs.", "response_time": response_time, "response_code": response_code})
        except requests.RequestException as e:
            results.append({"type": "API Vulnerability", "url": url, "payload": payload, "status": f"Error: {e}", "response_time": 0, "response_code": 0})

    results.append({"type": "API Vulnerability", "url": url, "status": "No vulnerability found", "response_time": 0, "response_code": 0})
    return results

def scan_exploitdb_vulnerabilities(url, exploits):
    """
    Scans the given URL for vulnerabilities listed in the ExploitDB dataset.
    """
    results = []
    try:
        response = requests.get(url, headers={"User-Agent": USER_AGENT})
        response_time = response.elapsed.total_seconds()
        response_code = response.status_code
        page_content = response.text.lower()
        for exploit in exploits:
            if exploit['platform'] in page_content or exploit['description'].lower() in page_content:
                results.append({
                    "type": "ExploitDB Vulnerability",
                    "url": url,
                    "exploit_id": exploit['id'],
                    "description": exploit['description'],
                    "platform": exploit['platform'],
                    "port": exploit['port'],
                    "codes": exploit['codes'],
                    "status": "Potential vulnerability found",
                    "response_time": response_time,
                    "response_code": response_code
                })
    except requests.RequestException as e:
        results.append({"type": "ExploitDB Vulnerability", "url": url, "status": f"Error: {e}", "response_time": 0, "response_code": 0})
    return results

def scan_url(url, user_agent, login_url=None):
    print(f"Scanning {url} for vulnerabilities...")
    results = []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(information_gathering, url),
            executor.submit(scan_sql_injection, url),
            executor.submit(scan_xss, url),
            executor.submit(scan_security_misconfigurations, url),
            executor.submit(scan_broken_authentication, url, login_url),
            executor.submit(scan_session_management, url),
            executor.submit(scan_nmap, url),
            executor.submit(scan_vulnerable_components, url),
            executor.submit(scan_identification_and_authentication_failures, url),
            executor.submit(scan_software_and_data_integrity_failures, url),
            executor.submit(scan_security_logging_and_monitoring_failures, url),
            executor.submit(scan_ssrf, url),
            executor.submit(scan_api_vulnerabilities, url),
            executor.submit(scan_exploitdb_vulnerabilities, url, exploits),  # Add the new function to the scan
        ]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            print(f"Result: {result}")
            results.extend(result)
            time.sleep(1)  # Add a delay between requests
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Web Vulnerability Scanner")
    parser.add_argument("url", help="The URL to scan for vulnerabilities")
    parser.add_argument("--user-agent", default="HackerOne + your H1 username", help="The User-Agent string to use in requests")
    parser.add_argument("--output", help="The file to save the scan results in JSON format")
    parser.add_argument("--login-url", help="The login URL to use for Broken Authentication scanning")
    args = parser.parse_args()

    target_url = args.url
    user_agent = args.user_agent
    output_file = args.output
    login_url = args.login_url

    # Initialize the exploits variable
    csv_file = '/home/ubuntu/files_exploits.csv'
    exploits = parse_exploits(csv_file)

    scan_results = scan_url(target_url, user_agent, login_url)
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(scan_results, f, indent=4)
        print(f"Scan results saved to {output_file}")
    else:
        print(json.dumps(scan_results, indent=4))
